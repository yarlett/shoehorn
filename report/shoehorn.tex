\documentclass[11pt]{article} %amsart
\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
%\geometry{landscape}                % Activate for for rotated page geometry
\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{epstopdf}
\usepackage{amsmath}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

\title{Nonlinear Dimensional Reduction Via Nearest Neighbor Reconstruction}
\author{Daniel Yarlett}
\date{}

\begin{document}
\maketitle

\section{Introduction}

This report describes a novel model of nonlinear dimensional reduction. The model applies to objects that can be represented as high-dimensional discrete probability distributions, and operates by embedding these high-dimensional objects in a low-dimensional space of selected dimensionality such that each high-dimensional object can be optimally reconstructed based on a weighted sum over the nearest neighbors of the object in the low-dimensional space. The weight function decays exponentially with increasing distance in the low-dimensional space which encourages the algorithm to locate `similar' objects near to one another in the low-dimensional space.

We demonstrate the effectiveness of the model on the MNIST Database of Handwritten Digits (LeCun, Bottou, Bengio and Haffner, 1998). This data set consists of 70,000 digitized images of handwritten numeric digits which have been size-normalized and frame-centered, and where each pixel is represented by a monochromatic intensity level. The model proves effective in locating digits belonging to the same class in tight clusters in a 2-dimensional space despite being presented with unlabeled data, indicating that the model could assist in applications such as the visualization and classification of high-dimensional data.

\section{The Model}

We assume that each object to be modeled can be represented by a discrete probability distribution $P^{(m)}$, consisting of elements $p^{(m)}_i$ denoting the probability of feature $i$ under the object, $m$, where $\sum_i p^{(m)}_i=1$.

The criterion the model optimizes is the error arising from reconstructing the probability distribution associated with a given object based on a weighted sum over the probability distributions associated with the nearest neighbors of the object in a postulated low dimensional manifold. We measure the reconstruction error on feature $i$ of object $m$, $E^{(m)}_i$, using the Kullback-Leibler metric
\begin{equation*}
E^{(m)}_i \equiv p^{(m)}_i \log{\frac{p^{(m)}_i}{q^{(m)}_i}} = p^{(m)}_i \log p^{(m)}_i - p^{(m)}_i \log q^{(m)}_i
\tag{1}
\end{equation*}
where $q^{(m)}_i$ denotes the reconstructed probability estimate of $p^{(m)}_i$ under the model. This `reconstructed' probability estimate is generated by a weighted sum over the probabilities assigned to feature $i$ by the nearest neighbors of object $m$ in the low dimensional manifold, defined as follows
\begin{equation*}
q^{(m)}_i \equiv \alpha p^{(m)}_i + (1 - \alpha)\frac{\sum_{n \in N}w_{nm}p^{(n)}_i}{\sum_{n \in N}w_{nm}}
\tag{2}
\end{equation*}
where $0 < \alpha < 1$, and $N$ is the set of all objects that are the nearest neighbors of object $m$. The nearest neighbor set for an object is defined by measuring the Euclidean distance of all the objects from the target object, and then taking the $k$ nearest objects. Note that an object cannot be a nearest neighbor of itself.

The $\alpha$ parameter employed above is intended to prevent the reconstructed probability from equaling 0 in the event that none of the nearest neighbors of $m$ assign a non-zero probability to feature $i$ (which would cause the logarithm in Equation (1) to become degenerate); in most applications we have found that $\alpha \approx 0.01$ leads to good performance.

$w_{nm}$ denotes the weight assigned to object $n$ relative to object $m$ in the sum, and is a decreasing function of the distance between the two objects in the low dimensional manifold as follows
\begin{equation*}
w_{nm} \equiv \exp(-d_{nm}^{\frac{1}{2}})
\tag{3}
\end{equation*}
where 
\begin{equation*}
d_{nm} \equiv (x^{(m)} - x^{(n)})^2 + (y^{(m)} - y^{(n)})^2 + ...
\tag{4}
\end{equation*}
and $x^{(m)}$ represents the position of object $m$ on dimension $x$, $y^{(m)}$ represents the position of object $m$ on dimension $y$, and so on, for each dimension of the low-dimensional space.

\section{Learning Low-Dimensional Locations}

The locations assigned to the objects in the postulated low-dimensional space are learned by setting them such that they minimize the sum of the reconstruction error in Equation (1) for each feature of each object in the data set, $\sum_{m, i} E^{(m)}_i$.

In order to facilitate the optimization process we here derive the rate of change of $E^{(m)}_i$ with respect to its location on an arbitrary dimension, $x$, of the low dimensional manifold. The derived gradient equation can be used in order to minimize the overall reconstruction error through a process of iterative gradient-descent.

Substituting Equation (2) into Equation (1) we get
\begin{equation*}
E^{(m)}_i \equiv p^{(m)}_i \log p^{(m)}_i - p^{(m)}_i \log \left( \alpha p^{(m)}_i + (1 - \alpha)\frac{\sum_{n \in N}w_{nm}p^{(n)}_i}{\sum_{n \in N}w_{nm}} \right)
\end{equation*}
And differentiating this equation yields
\begin{align*}
\frac{dE^{(m)}_i}{dx^{(m)}} &=
(\alpha - 1)p^{(m)}_i \left( \alpha p^{(m)}_i + (1 - \alpha)\frac{\sum_{n \in N}w_{nm}p^{(n)}_i}{\sum_{n \in N}w_{nm}} \right)^{-1} \frac{d}{dx^{(m)}}\left( \frac{\sum_{n \in N}w_{nm}p^{(n)}_i}{\sum_{n \in N}w_{nm}} \right)\\
&= \frac{(\alpha - 1)p^{(m)}_i}{q^{(m)}_i}\frac{d}{dx^{(m)}}\left( \frac{\sum_{n \in N}w_{nm}p^{(n)}_i}{\sum_{n \in N}w_{nm}} \right)
\tag{5}
\end{align*}
We now differentiate the final term in Equation (5), by replacing the $w_{nm}$ terms with their definition in Equation (3) and then applying the quotient rule
\begin{align*}
\frac{d}{dx^{(m)}_i}\left(\frac{\sum_{n \in N}w_{nm}p^{(n)}_i}{\sum_{n \in N}w_{nm}}\right) = \frac{d}{dx^{(m)}_i}\left(\frac{\sum_{n \in N}\exp(-d_{nm}^{\frac{1}{2}})p^{(n)}_i}{\sum_{n \in N}\exp(-d_{nm}^{\frac{1}{2}})}\right)\\
= \frac{\left(\sum_{n \in N}\left(x^{(n)}-x^{(m)}\right)\exp(-d_{nm}^{\frac{1}{2}})d^{-\frac{1}{2}}p^{(n)}_i\right)\left(\sum_{n \in N}\exp(-d_{nm}^{\frac{1}{2}})\right)}{\left(\sum_{n \in N}\exp(-d_{nm}^{\frac{1}{2}})\right)^2}\\
- \frac{\left(\sum_{n \in N}\exp(-d_{nm}^{\frac{1}{2}})p^{(n)}_i\right)\left(\sum_{n \in N}\left(x^{(n)}-x^{(m)}\right)\exp(-d_{nm}^{\frac{1}{2}})d_{nm}^{-\frac{1}{2}}\right)}{\left(\sum_{n \in N}\exp(-d_{nm}^{\frac{1}{2}})\right)^2}
\tag{6}
\end{align*}
By substituting Equation (6) into Equation (5) we get the final expression for the gradient we have been seeking
\begin{equation*}
\frac{dE^{(m)}_i}{dx^{(m)}} = \frac{(\alpha - 1)p^{(m)}_i}{q^{(m)}_i} \left(\frac{\left(\sum_n v_{nm}w_{nm}d_{nm}^{-\frac{1}{2}}p^{(n)}_i\right)\biggl(\sum_n w_{nm}\biggr) - \left(\sum_n w_{nm}p^{(n)}_i\right)\left(\sum_n v_{nm}w_{nm}d_{nm}^{-\frac{1}{2}}\right)}{\biggl(\sum_n w_{nm}\biggr)^2}\right)
\tag{7}
\end{equation*}
where $v_{nm} \equiv x^{(n)} - x^{(m)}$.
\end{document}  